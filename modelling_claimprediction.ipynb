{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29f311c0",
   "metadata": {},
   "source": [
    "# Coding Challenge Huk-Coburg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48117d9d",
   "metadata": {},
   "source": [
    "Experiment to predict whether a Claim will be raised from the features of the data. \n",
    "\n",
    "\n",
    "#### Metric\n",
    "As metric I choose F1, as it is the standard metric for classification tasks with biased data as it combines precision and recall. Additionally the confusion matrix offers an interesting picture of the nature of the errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dac15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "\n",
    "mlflow.sklearn.autolog()\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "mlflow.set_experiment(\"ClaimsClassification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dcacfa",
   "metadata": {},
   "source": [
    "## Data Preparation - Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce4778e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d77512",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"data/df_full.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a76270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ClaimNorm, ClaimAmount and exposure are dependent variables of ClaimNorm \n",
    "df = df.drop([\"ClaimNorm\", \"ClaimAmount\", \"Exposure\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba70300",
   "metadata": {},
   "source": [
    "I removed exposure, since a longer exposure increases the probability of a claim (see correaltion). A deeper analysis should correct for this fact, e.g. by training with data points with equal exposure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b57c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transorm to a binary task\n",
    "labels = df[\"ClaimNb\"].apply(lambda n: int(bool(n))).to_numpy()\n",
    "data = df.drop(\"ClaimNb\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834dfc79",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c6c45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test, labels_train, labels_test = train_test_split(data, labels, test_size=0.1, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9d76e1",
   "metadata": {},
   "source": [
    "#### Is the split unbiased (similar ratio of no-claims)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3480bf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ratio no-claims in Test\", sum(labels_test > 0)/len(labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec9f4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ratio no-claims in Train\", sum(labels_train > 0)/len(labels_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e48b3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d741ebb4",
   "metadata": {},
   "source": [
    "Here the Baseline is the Dummy model classifing everything as a claim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bc2c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "with mlflow.start_run(run_name=\"Baseline\"):\n",
    "    dummy_model = DummyClassifier(strategy=\"constant\", constant=1)\n",
    "    dummy_model.fit(data_train, labels_train)\n",
    "    dummy_model.score(data_test, labels_test)\n",
    "    labels_pred = dummy_model.predict(data_test)\n",
    "    f1_score(labels_test, labels_pred)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e8bc47",
   "metadata": {},
   "source": [
    "## Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef28e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bbbac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"LogistRegression\"):\n",
    "    logistic_model = LogisticRegression()\n",
    "    logistic_model.fit(data_train, labels_train)\n",
    "    logistic_model.score(data_test, labels_test)\n",
    "    labels_pred = logistic_model.predict(data_test)\n",
    "    f1_score(labels_test, labels_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b13082",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1100752",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c7fbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"RandomForest-weights100\"):\n",
    "    model = RandomForestClassifier(class_weight={0:1,1:100})\n",
    "    model.fit(data_train, labels_train)\n",
    "    model.score(data_test, labels_test)\n",
    "    labels_pred = model.predict(data_test)\n",
    "    f1_score(labels_test, labels_pred)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels_test, labels_pred).ravel()\n",
    "    mlflow.log_metrics({\"TN\":tn, \"FP\":fp, \"FN\":fn,\"TP\":tp})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913f9c3d",
   "metadata": {},
   "source": [
    "## Class Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce899bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610ce50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_full = data_train\n",
    "data_train_full[\"label\"] = labels_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6c7cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_neg = data_train_full[data_train_full[\"label\"]==0]\n",
    "data_pos = data_train_full[data_train_full[\"label\"]==1]\n",
    "n_pos=len(data_pos)\n",
    "data_neg_resample = resample(data_neg, n_samples=n_pos, replace=False)\n",
    "data_resample = pd.concat([data_neg_resample, data_pos])\n",
    "data_train_resample = data_resample.drop(\"label\", axis=1)\n",
    "label_train_resample = data_resample[\"label\"].to_numpy()\n",
    "sum(label_train_resample) / len(label_train_resample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09158a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"RandomForest-downsample\"):\n",
    "    model = RandomForestClassifier(class_weight={0:1,1:100})\n",
    "    model.fit(data_train_resample, label_train_resample)\n",
    "    model.score(data_test, labels_test)\n",
    "    labels_pred = model.predict(data_test)\n",
    "    f1_score(labels_test, labels_pred)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels_test, labels_pred).ravel()\n",
    "    mlflow.log_metrics({\"TN\":tn, \"FP\":fp, \"FN\":fn,\"TP\":tp})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b85f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"LogistRegression-downsample\"):\n",
    "    logistic_model = LogisticRegression()\n",
    "    logistic_model.fit(data_train_resample, label_train_resample)\n",
    "    logistic_model.score(data_test, labels_test)\n",
    "    labels_pred = logistic_model.predict(data_test)\n",
    "    f1_score(labels_test, labels_pred)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels_test, labels_pred).ravel()\n",
    "    mlflow.log_metrics({\"TN\":tn, \"FP\":fp, \"FN\":fn,\"TP\":tp})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e18337",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import uniform\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "scaler = StandardScaler()\n",
    "data_train_resample_scale = scaler.fit_transform(data_train_resample)\n",
    "\n",
    "model = LogisticRegression()\n",
    "distributions = dict(tol=[1e-4, 5e-4, 1e-3, 5e-3],\n",
    "                     C=uniform(loc=0.1, scale=2),\n",
    "                     max_iter=[50,100,150,200]\n",
    "                     )\n",
    "\n",
    "with mlflow.start_run(run_name=\"HT-LogistRegression-scaler-downsample\"):\n",
    "    randsearch = RandomizedSearchCV(model, distributions, random_state=0)\n",
    "    search = randsearch.fit(data_train_resample_scale, label_train_resample)\n",
    "    data_test_scale = scaler.transform(data_test)\n",
    "    labels_pred = search.predict(data_test_scale)\n",
    "    f1_score(labels_test, labels_pred)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels_test, labels_pred).ravel()\n",
    "    mlflow.log_metrics({\"TN\":tn, \"FP\":fp, \"FN\":fn,\"TP\":tp})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a211b71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"RandomForest-downsample-equal\"):\n",
    "    model = RandomForestClassifier()\n",
    "    model.fit(data_train_resample, label_train_resample)\n",
    "    model.score(data_test, labels_test)\n",
    "    labels_pred = model.predict(data_test)\n",
    "    f1_score(labels_test, labels_pred)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels_test, labels_pred).ravel()\n",
    "    mlflow.log_metrics({\"TN\":tn, \"FP\":fp, \"FN\":fn,\"TP\":tp})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e1617f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a469929",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "45f74486",
   "metadata": {},
   "source": [
    "## Feature Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61e1f64",
   "metadata": {},
   "source": [
    "Which features are most relevant for the prediction of a likely claim?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80666649",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(data.columns, search.best_estimator_.coef_.ravel()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb0c93c",
   "metadata": {},
   "source": [
    "Check typical probability predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afd62c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "search.best_estimator_.predict_proba(data_test_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebc23b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(labels_test, labels_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
